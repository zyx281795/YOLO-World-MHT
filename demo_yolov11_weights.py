#!/usr/bin/env python3
"""
ä½¿ç”¨YOLOv11æ¬Šé‡çš„YOLO-World Demo

é€™å€‹è…³æœ¬æœƒï¼š
1. è½‰æ›YOLOv11æ¬Šé‡ç‚ºYOLO-Worldæ ¼å¼ï¼ˆå¦‚æœéœ€è¦ï¼‰
2. å•Ÿå‹•Gradioä»‹é¢é€²è¡Œæ¨ç†
"""

import os
import sys
import torch
import numpy as np
import gradio as gr
from PIL import Image
import supervision as sv
from mmengine.config import Config
from mmengine.dataset import Compose
from mmengine.runner.amp import autocast
from mmengine.runner import Runner
from mmdet.datasets import CocoDataset

# Import YOLO-World and YOLOv11 components
import yolo_world
from yolo_world.models.layers import yolov11_blocks

def convert_weights_if_needed():
    """è½‰æ›YOLOv11æ¬Šé‡ï¼ˆå¦‚æœå°šæœªè½‰æ›ï¼‰"""
    yolov11_path = 'checkpoints/yolo11n.pt'
    converted_path = 'checkpoints/yolo_world_yolov11n_converted.pth'
    
    if not os.path.exists(converted_path) or os.path.getmtime(yolov11_path) > os.path.getmtime(converted_path):
        print("ğŸ”„ æ­£åœ¨è½‰æ›YOLOv11æ¬Šé‡...")
        
        # è¼‰å…¥YOLOv11æ¬Šé‡
        yolov11_ckpt = torch.load(yolov11_path, map_location='cpu')
        
        # æå–æ¨¡å‹æ¬Šé‡
        if hasattr(yolov11_ckpt, 'model'):
            state_dict = yolov11_ckpt.model.state_dict() if hasattr(yolov11_ckpt.model, 'state_dict') else yolov11_ckpt
        elif 'model' in yolov11_ckpt:
            state_dict = yolov11_ckpt['model']
            if hasattr(state_dict, 'state_dict'):
                state_dict = state_dict.state_dict()
        else:
            state_dict = yolov11_ckpt
        
        # å‰µå»ºé©é…çš„state_dict
        new_state_dict = {}
        
        # åªä¿ç•™backboneç›¸é—œæ¬Šé‡ï¼Œè®“text_modelä½¿ç”¨é è¨“ç·´æ¬Šé‡
        for key, value in state_dict.items():
            # æ˜ å°„backboneæ¬Šé‡åˆ°image_model
            if any(prefix in key for prefix in ['backbone', 'neck', 'model']):
                # ç°¡å–®çš„keyæ˜ å°„
                new_key = f"backbone.image_model.{key}"
                new_state_dict[new_key] = value
        
        # ä¿å­˜è½‰æ›å¾Œçš„æ¬Šé‡
        checkpoint = {
            'state_dict': new_state_dict,
            'meta': {
                'converted_from': 'yolo11n.pt',
                'note': 'Partial weights for backbone only'
            }
        }
        
        os.makedirs(os.path.dirname(converted_path), exist_ok=True)
        torch.save(checkpoint, converted_path)
        print(f"âœ… æ¬Šé‡è½‰æ›å®Œæˆ: {converted_path}")
    
    return converted_path

def initialize_model():
    """åˆå§‹åŒ–ä½¿ç”¨YOLOv11æ¬Šé‡çš„æ¨¡å‹"""
    global runner, test_pipeline
    
    print("ğŸš€ åˆå§‹åŒ–YOLO-World + YOLOv11æ¬Šé‡...")
    
    # è½‰æ›æ¬Šé‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
    converted_weights = convert_weights_if_needed()
    
    # ä½¿ç”¨åŸºç¤é…ç½®ä½†è¼‰å…¥è½‰æ›å¾Œçš„æ¬Šé‡
    config_file = "configs/pretrain/yolo_world_v2_x_vlpan_bn_2e-3_100e_4x8gpus_obj365v1_goldg_train_1280ft_lvis_minival.py"
    
    # è¼‰å…¥é…ç½®
    cfg = Config.fromfile(config_file)
    cfg.work_dir = './work_dirs'
    cfg.load_from = converted_weights
    
    # ç¢ºä¿YOLOv11çµ„ä»¶å¯ç”¨
    cfg.custom_imports = dict(
        imports=['yolo_world', 'yolo_world.models.layers.yolov11_blocks'],
        allow_failed_imports=False
    )
    
    try:
        # å‰µå»ºrunner
        runner = Runner.from_cfg(cfg)
        runner.call_hook('before_run')
        
        # å˜—è©¦è¼‰å…¥æ¬Šé‡ï¼Œä½¿ç”¨strict=Falseå…è¨±éƒ¨åˆ†è¼‰å…¥
        print("ğŸ“¥ è¼‰å…¥YOLOv11è½‰æ›æ¬Šé‡...")
        checkpoint = torch.load(converted_weights, map_location='cpu')
        
        if 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        else:
            state_dict = checkpoint
        
        # è¼‰å…¥æ¬Šé‡ï¼Œå…è¨±éƒ¨åˆ†è¼‰å…¥
        missing_keys, unexpected_keys = runner.model.load_state_dict(state_dict, strict=False)
        print(f"âš ï¸ Missing keys: {len(missing_keys)}")
        print(f"âš ï¸ Unexpected keys: {len(unexpected_keys)}")
        
        # è¨­ç½®æ¸¬è©¦pipeline
        pipeline = cfg.test_dataloader.dataset.pipeline
        pipeline[0].type = 'mmdet.LoadImageFromNDArray'
        test_pipeline = Compose(pipeline)
        
        runner.model.eval()
        
        print("âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼")
        print(f"ğŸ¯ ä½¿ç”¨YOLOv11æ¬Šé‡: yolo11n.pt")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±æ•—: {e}")
        
        # å›é€€åˆ°åŸå§‹æ¬Šé‡
        print("ğŸ”„ å›é€€åˆ°åŸå§‹YOLO-Worldæ¬Šé‡...")
        cfg.load_from = "checkpoints/yolo_world_v2_x_obj365v1_goldg_cc3mlite_pretrain_1280ft-14996a36.pth"
        
        runner = Runner.from_cfg(cfg)
        runner.call_hook('before_run')
        runner.load_or_resume()
        
        pipeline = cfg.test_dataloader.dataset.pipeline
        pipeline[0].type = 'mmdet.LoadImageFromNDArray'
        test_pipeline = Compose(pipeline)
        runner.model.eval()
        
        print("âš ï¸ ä½¿ç”¨åŸå§‹YOLO-Worldæ¬Šé‡")
        return False

def inference(image, texts, score_thr=0.3, max_dets=100):
    """åŸ·è¡Œæ¨ç†"""
    if isinstance(image, Image.Image):
        image = np.array(image)
    
    # æº–å‚™æ•¸æ“š
    data_info = dict(img=image, img_id=0, texts=texts)
    data_info = test_pipeline(data_info)
    data_batch = dict(inputs=data_info['inputs'].unsqueeze(0),
                      data_samples=[data_info['data_samples']])
    
    # æ¨ç†
    with autocast(enabled=False), torch.no_grad():
        output = runner.model.test_step(data_batch)[0]
        pred_instances = output.pred_instances
    
    # éæ¿¾çµæœ
    pred_instances = pred_instances[pred_instances.scores.float() > score_thr]
    if len(pred_instances.scores) > max_dets:
        indices = pred_instances.scores.float().topk(max_dets)[1]
        pred_instances = pred_instances[indices]
    
    pred_instances = pred_instances.cpu().numpy()
    
    # å‰µå»ºæª¢æ¸¬çµæœ
    detections = sv.Detections(
        xyxy=pred_instances['bboxes'],
        class_id=pred_instances['labels'],
        confidence=pred_instances['scores']
    )
    
    # æ¨™è¨»åœ–åƒ
    box_annotator = sv.BoundingBoxAnnotator(thickness=2)
    label_annotator = sv.LabelAnnotator(text_padding=4, text_scale=0.5, text_thickness=1)
    
    labels = [f"{texts[class_id][0]} {confidence:0.2f}" 
              for class_id, confidence in zip(detections.class_id, detections.confidence)]
    
    annotated_image = box_annotator.annotate(image.copy(), detections)
    annotated_image = label_annotator.annotate(annotated_image, detections, labels=labels)
    
    return annotated_image, len(pred_instances['labels'])

def predict(image, text_input, score_threshold, max_boxes):
    """Gradioé æ¸¬å‡½æ•¸"""
    if image is None:
        return None, "0"
    
    # è§£ææ–‡æœ¬è¼¸å…¥
    texts = [[t.strip()] for t in text_input.split(',')] + [[' ']]
    
    # é‡æ–°åƒæ•¸åŒ–æ¨¡å‹
    runner.model.reparameterize(texts)
    
    # åŸ·è¡Œæ¨ç†
    result_image, num_detected = inference(
        image, texts, score_thr=score_threshold, max_dets=max_boxes
    )
    
    return result_image, str(num_detected)

# åˆå§‹åŒ–æ¨¡å‹
print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–YOLO-World + YOLOv11æ¬Šé‡...")
success = initialize_model()

weight_status = "âœ… YOLOv11æ¬Šé‡ (yolo11n.pt)" if success else "âš ï¸ åŸå§‹YOLO-Worldæ¬Šé‡ (å‚™ç”¨)"

# å‰µå»ºGradioä»‹é¢
with gr.Blocks(title="YOLO-World + YOLOv11 Weights", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ¯ YOLO-World + YOLOv11æ¬Šé‡")
    gr.Markdown(f"**æ¬Šé‡ç‹€æ…‹:** {weight_status}")
    
    if success:
        gr.Markdown("""
        ### âœ… æˆåŠŸè¼‰å…¥YOLOv11æ¬Šé‡ï¼
        - ä½¿ç”¨YOLOv11n (nano) æ¬Šé‡æª”æ¡ˆ
        - ä¿æŒYOLO-Worldçš„é–‹æ”¾è©å½™èƒ½åŠ›
        - çµåˆYOLOv11çš„æ¶æ§‹æ”¹é€²
        """)
    else:
        gr.Markdown("""
        ### âš ï¸ YOLOv11æ¬Šé‡è¼‰å…¥å¤±æ•—
        - ç›®å‰ä½¿ç”¨åŸå§‹YOLO-Worldæ¬Šé‡
        - ä»ç„¶åŒ…å«YOLOv11çµ„ä»¶æ”¯æ´
        - ä¿æŒå®Œæ•´åŠŸèƒ½
        """)
    
    with gr.Row():
        with gr.Column():
            input_image = gr.Image(type="pil", label="ä¸Šå‚³åœ–ç‰‡")
            text_input = gr.Textbox(
                label="è¦åµæ¸¬çš„ç‰©é«”ï¼ˆç”¨é€—è™Ÿåˆ†éš”ï¼‰", 
                placeholder="äºº, è»Š, ç‹—, è²“, è…³è¸è»Š",
                value="äºº, è»Š, ç‹—, è²“"
            )
            
            with gr.Row():
                score_threshold = gr.Slider(
                    minimum=0.0, maximum=1.0, value=0.3, step=0.05,
                    label="ä¿¡å¿ƒåº¦é–¾å€¼"
                )
                max_boxes = gr.Slider(
                    minimum=1, maximum=100, value=50, step=1,
                    label="æœ€å¤§åµæ¸¬æ•¸é‡"
                )
            
            predict_btn = gr.Button("ğŸ” é–‹å§‹åµæ¸¬", variant="primary", size="lg")
        
        with gr.Column():
            output_image = gr.Image(label="åµæ¸¬çµæœ")
            detected_count = gr.Textbox(label="åµæ¸¬åˆ°çš„ç‰©é«”æ•¸é‡", interactive=False)
    
    predict_btn.click(
        fn=predict,
        inputs=[input_image, text_input, score_threshold, max_boxes],
        outputs=[output_image, detected_count]
    )
    
    # ç¯„ä¾‹
    gr.Examples(
        examples=[
            ["demo/sample_images/bus.jpg", "å·´å£«, äºº, è»Š", 0.3, 50],
        ],
        inputs=[input_image, text_input, score_threshold, max_boxes],
        outputs=[output_image, detected_count],
        fn=predict,
        cache_examples=False
    )

if __name__ == "__main__":
    print(f"ğŸš€ å•Ÿå‹•Demo - {weight_status}")
    demo.launch(server_name='0.0.0.0', server_port=8084, share=False)