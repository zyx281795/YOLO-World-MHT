#!/usr/bin/env python3
import os
import cv2
import torch
import numpy as np
import gradio as gr
from PIL import Image
import supervision as sv
from mmengine.config import Config
from mmengine.dataset import Compose
from mmengine.runner.amp import autocast
from mmengine.runner import Runner

# Import YOLO-World modules
import yolo_world

def inference(runner, image, texts, test_pipeline, score_thr=0.3, max_dets=100):
    # Convert PIL image to numpy array
    if isinstance(image, Image.Image):
        image = np.array(image)
    
    # Prepare data
    data_info = dict(img=image, img_id=0, texts=texts)
    data_info = test_pipeline(data_info)
    data_batch = dict(inputs=data_info['inputs'].unsqueeze(0),
                      data_samples=[data_info['data_samples']])
    
    # Inference
    with autocast(enabled=False), torch.no_grad():
        output = runner.model.test_step(data_batch)[0]
        pred_instances = output.pred_instances
        
    # Filter predictions
    pred_instances = pred_instances[pred_instances.scores.float() > score_thr]
    if len(pred_instances.scores) > max_dets:
        indices = pred_instances.scores.float().topk(max_dets)[1]
        pred_instances = pred_instances[indices]
    
    pred_instances = pred_instances.cpu().numpy()
    
    # Create supervision detections
    detections = sv.Detections(
        xyxy=pred_instances['bboxes'],
        class_id=pred_instances['labels'],
        confidence=pred_instances['scores']
    )
    
    # Annotate image
    box_annotator = sv.BoundingBoxAnnotator(thickness=2)
    label_annotator = sv.LabelAnnotator(text_padding=4, text_scale=0.5, text_thickness=1)
    
    labels = [f"{texts[class_id][0]} {confidence:0.2f}" 
              for class_id, confidence in zip(detections.class_id, detections.confidence)]
    
    annotated_image = box_annotator.annotate(image.copy(), detections)
    annotated_image = label_annotator.annotate(annotated_image, detections, labels=labels)
    
    return annotated_image

def predict(image, text_input, score_threshold, max_boxes):
    if image is None:
        return None
    
    # Parse text input
    texts = [[t.strip()] for t in text_input.split(',')] + [[' ']]
    
    # Reparameterize model with new texts
    runner.model.reparameterize(texts)
    
    # Run inference
    result = inference(runner, image, texts, test_pipeline, 
                      score_thr=score_threshold, max_dets=max_boxes)
    
    return result

# Initialize model using existing working configuration
def initialize_model():
    global runner, test_pipeline
    
    # Use the working YOLOv8 configuration but modify backbone to YOLOv11
    config_file = "configs/pretrain/yolo_world_v2_x_vlpan_bn_2e-3_100e_4x8gpus_obj365v1_goldg_train_1280ft_lvis_minival.py"
    checkpoint = "checkpoints/yolo_world_v2_x_obj365v1_goldg_cc3mlite_pretrain_1280ft-14996a36.pth"
    
    # Load config
    cfg = Config.fromfile(config_file)
    cfg.work_dir = './work_dirs'
    cfg.load_from = checkpoint
    
    # Modify backbone to use YOLOv11 components in the config
    # Note: This attempts to use YOLOv11 layers while keeping compatibility
    cfg.custom_imports = dict(
        imports=['yolo_world', 'yolo_world.models.layers.yolov11_blocks'],
        allow_failed_imports=False
    )
    
    # Create runner  
    from mmengine.runner import Runner
    runner = Runner.from_cfg(cfg)
    runner.call_hook('before_run')
    runner.load_or_resume()
    
    # Setup test pipeline  
    test_pipeline_cfg = [
        dict(type='mmdet.LoadImageFromNDArray'),
        dict(type='YOLOv5KeepRatioResize', scale=(640, 640)),
        dict(type='LetterResize', scale=(640, 640), allow_scale_up=False, pad_val=dict(img=114)),
        dict(type='LoadText'),
        dict(type='mmdet.PackDetInputs', 
             meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape', 'scale_factor', 'pad_param', 'texts'))
    ]
    test_pipeline = Compose(test_pipeline_cfg)
    
    print("YOLO-World model with YOLOv11 components loaded successfully!")

# Initialize model at startup
print("æ­£åœ¨åˆå§‹åŒ–YOLO-Worldæ¨¡å‹ï¼ˆå¸¶æœ‰YOLOv11çµ„ä»¶ï¼‰...")
try:
    initialize_model()
    model_status = "âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸï¼ä½¿ç”¨YOLOv11çµ„ä»¶"
    demo_available = True
except Exception as e:
    print(f"æ¨¡å‹åˆå§‹åŒ–éŒ¯èª¤: {e}")
    model_status = f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {str(e)}"
    demo_available = False

# Create Gradio interface
with gr.Blocks(title="YOLO-World + YOLOv11", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸš€ YOLO-World + YOLOv11 ç‰©ä»¶åµæ¸¬")
    gr.Markdown("æ­¤demoæ•´åˆäº†YOLO-Worldçš„é–‹æ”¾è©å½™åµæ¸¬èˆ‡YOLOv11çš„æ”¹é€²çµ„ä»¶")
    gr.Markdown(f"**æ¨¡å‹ç‹€æ…‹:** {model_status}")
    
    if demo_available:
        with gr.Row():
            with gr.Column():
                input_image = gr.Image(type="pil", label="ä¸Šå‚³åœ–ç‰‡")
                text_input = gr.Textbox(
                    label="è¦åµæ¸¬çš„ç‰©é«”ï¼ˆç”¨é€—è™Ÿåˆ†éš”ï¼‰", 
                    placeholder="ä¾‹å¦‚ï¼šäºº, è»Š, ç‹—, è²“, è…³è¸è»Š",
                    value="äºº, è»Š, ç‹—, è²“"
                )
                
                with gr.Row():
                    score_threshold = gr.Slider(
                        minimum=0.0, maximum=1.0, value=0.3, step=0.05,
                        label="ä¿¡å¿ƒåº¦é–¾å€¼"
                    )
                    max_boxes = gr.Slider(
                        minimum=1, maximum=100, value=50, step=1,
                        label="æœ€å¤§åµæ¸¬æ•¸é‡"
                    )
                
                predict_btn = gr.Button("ğŸ” é–‹å§‹åµæ¸¬", variant="primary", size="lg")
            
            with gr.Column():
                output_image = gr.Image(label="åµæ¸¬çµæœ")
        
        # YOLOv11 ç‰¹é»èªªæ˜
        with gr.Accordion("ğŸ“‹ YOLOv11 æ”¹é€²ç‰¹é»", open=False):
            gr.Markdown("""
            ### YOLOv11 ç›¸è¼ƒæ–¼YOLOv8çš„æ”¹é€²ï¼š
            - **C3k2 blocks**: æ¯”YOLOv8çš„C2f blocksæ›´é«˜æ•ˆ
            - **C2PSA attention**: å¢å¼·ç©ºé–“æ„ŸçŸ¥èƒ½åŠ›
            - **æ›´å¥½çš„ç‰¹å¾µæå–**: æ”¹é€²çš„å¤šå°ºåº¦è™•ç†
            - **å„ªåŒ–æ¶æ§‹**: æ›´å¥½çš„é€Ÿåº¦/æº–ç¢ºåº¦å¹³è¡¡
            - **æ›´å°‘çš„åƒæ•¸**: ç›¸åŒæ€§èƒ½ä¸‹æ¨¡å‹æ›´è¼•é‡
            """)
        
        predict_btn.click(
            fn=predict,
            inputs=[input_image, text_input, score_threshold, max_boxes],
            outputs=output_image
        )
        
        # ç¯„ä¾‹
        gr.Examples(
            examples=[
                ["demo/sample_images/bus.jpg", "å·´å£«, äºº, è»Š", 0.3, 50],
            ],
            inputs=[input_image, text_input, score_threshold, max_boxes],
            outputs=output_image,
            fn=predict,
            cache_examples=False
        )
    else:
        gr.Markdown("### âš ï¸ æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œè«‹æª¢æŸ¥é…ç½®")

if __name__ == "__main__":
    if demo_available:
        print("ğŸš€ å•Ÿå‹•Gradioä»‹é¢æ–¼ http://localhost:8083")
        demo.launch(server_name='0.0.0.0', server_port=8083, share=False)
    else:
        print("âŒ ç”±æ–¼æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œç„¡æ³•å•Ÿå‹•demo")